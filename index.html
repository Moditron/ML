<!DOCTYPE html>
<html>
<head>
    <title>Python ML Snippets</title>
</head>
<body>
    <h1>Preprocessing</h1>
    <pre><code>import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from scipy import stats

# Create a DataFrame
df = pd.read_csv('Dataset.csv')

print(f"Raw input:\n{df}")

# Imputation
imputer = SimpleImputer(strategy='mean')
df[['mks1', 'mks2', 'mks3']] = imputer.fit_transform(df[['mks1', 'mks2', 'mks3']])




# Anomaly detection
z = stats.zscore(df[['mks1', 'mks2', 'mks3']])
threshold = 3
outlier = (abs(z) > threshold).any(axis=1)

df['is_outlier'] = outlier

# Standardization
scale = StandardScaler()
df[['mks1', 'mks2', 'mks3']] = scale.fit_transform(df[['mks1', 'mks2', 'mks3']])

# Normalization
scale = MinMaxScaler()
df[['mks1', 'mks2', 'mks3']] = scale.fit_transform(df[['mks1', 'mks2', 'mks3']])

print(f'Final preprocessed:\n{df}')


<h3>#without using sklearn libraries</h3>


import pandas as pd

# Create a DataFrame
df = pd.read_csv('Dataset.csv')



# Imputation (Replace NaN with mean)
df[['mks1', 'mks2', 'mks3']] = df[['mks1', 'mks2', 'mks3']].fillna(df[['mks1', 'mks2', 'mks3']].mean())

# Anomaly detection (using Z-Score)
def detect_outliers_zscore(data, threshold=3):
    z_scores = (data - data.mean()) / data.std()
    is_outlier = (abs(z_scores) > threshold).any(axis=1)
    return is_outlier

outlier = detect_outliers_zscore(df[['mks1', 'mks2', 'mks3']])
df['is_outlier'] = outlier

# Standardization (z-score)
def standardize(data):
    return (data - data.mean()) / data.std()

df[['mks1', 'mks2', 'mks3']] = standardize(df[['mks1', 'mks2', 'mks3']])

# Normalization (min-max scaling)
def normalize(data):
    return (data - data.min()) / (data.max() - data.min())

df[['mks1', 'mks2', 'mks3']] = normalize(df[['mks1', 'mks2', 'mks3']])

print(f'Final preprocessed without using libraries:\n{df}')</code></pre>
    
    <h1>Simple Linear Regression</h1>
    <pre><code>import matplotlib.pyplot as plt
import math
import pandas as pd

data = pd.read_csv('Dataset.csv')
x = data['Independent_column_name'].values
y = data['Dependent_column_name'].values

x_mean = sum(x) / len(x)
y_mean = sum(y) / len(y)

numerator = sum((xi - x_mean) * (yi - y_mean) for xi, yi in zip(x, y))
denominator = sum((xi - x_mean) ** 2 for xi in x)

m = numerator / denominator
b = y_mean - m * x_mean

def predict(x):
    return m * x + b

# Calculate RMSE
rmse = math.sqrt(sum((yi - predict(xi)) ** 2 for xi, yi in zip(x, y)) / len(x))

# Print slope, intercept, and RMSE
print(f'Slope (m): {m}')
print(f'Intercept (b): {b}')
print(f'RMSE: {rmse}')

new_x = int(input("Enter the value of x for testing the regression model: "))

print(f'Predicted value of y is: {predict(new_x)}')

plt.scatter(x, y, label='data points')

regression_line = [predict(val) for val in x]

plt.plot(x, regression_line, color='red', label='regression line')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()</code></pre>
    
    <h1>Multiple Linear Regression</h1>
    <pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# User-provided data
data = pd.read_csv('Dataset.csv')
X1 = np.array(data['Independent_Variable1_Name'].values)
X2 = np.array(data['Independent_Variable2_Name'].values)
y = np.array(data['Dependent_Variable_Name'].values)

# Concatenate the features and add a column of ones for the intercept
X = np.vstack((X1, X2, np.ones(X1.shape[0]))).T

# Calculate the coefficients using OLS

coefficients = np.linalg.pinv(X).dot(y)
# Extract the coefficients
coef1, coef2, intercept = coefficients[0], coefficients[1], coefficients[2]

print("Coefficient 1:", coef1)
print("Coefficient 2:", coef2)
print("Intercept:", intercept)

# Create prediction function
def predict(X1, X2):
    return coef1 * X1 + coef2 * X2 + intercept

# Calculate Root Mean Square Error (RMSE)
predictions = predict(X1, X2)
rmse = np.sqrt(np.mean((y - predictions) ** 2))

print("Root Mean Square Error (RMSE):", rmse)

new_x1 = int(input("Enter the value of x1 for testing the regression model: "))
new_x2 = int(input("Enter the value of x2 for testing the regression model: "))

print(f'Predicted value of y is: {predict(new_x1, new_x2)}')

# Create a grid for plotting
x1 = np.linspace(min(X1), max(X1), 10)
x2 = np.linspace(min(X2), max(X2), 15)
X1, X2 = np.meshgrid(x1, x2)
Y = predict(X1, X2)

# Plot the 3D surface
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X1, X2, y, color='red', marker='o', label='Data Points')
ax.plot_surface(X1, X2, Y, color='blue', alpha=0.5, label='Regression Plane')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Y')
plt.show()</code></pre>
    
    <h1>Logistic Regression</h1>
    <pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Sample data
data = pd.read_csv('Dataset.csv')
X = np.array(data[['Independent_Variable1_Name', 'Independent_Variable2_Name']].values)
y = data['Dependent_Variable_Name'].values

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Logistic regression training
def train_logistic_regression(X, y, learning_rate, num_iterations):
    num_samples, num_features = X.shape
    weights = np.zeros(num_features)
    bias = 0

    for _ in range(num_iterations):
        linear_model = np.dot(X, weights) + bias
        predictions = sigmoid(linear_model)

        # Update weights and bias
        dw = (1 / num_samples) * np.dot(X.T, (predictions - y))
        db = (1 / num_samples) * np.sum(predictions - y)
        weights -= learning_rate * dw
        bias -= learning_rate * db

    return weights, bias

# Train the logistic regression model
learning_rate = 0.01
num_iterations = 1000
trained_weights, trained_bias = train_logistic_regression(X, y, learning_rate, num_iterations)
print("Learned Weights:", trained_weights)
print("Learned Bias:", trained_bias)
# Make predictions
def predict(X, weights, bias):
    linear_model = np.dot(X, weights) + bias
    predictions = sigmoid(linear_model)
    return [1 if p>0.5 else 0 for p in predictions]

# Create a scatter plot of the data
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Class 0', color='blue')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Class 1', color='red')

# Define the decision boundary
x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01), np.arange(x2_min, x2_max, 0.01))
Z = np.array(predict(np.c_[xx1.ravel(), xx2.ravel()], trained_weights, trained_bias))
Z = Z.reshape(xx1.shape)

# Plot the decision boundary
plt.contourf(xx1, xx2, Z, alpha=0.3, cmap='coolwarm')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

plt.legend()
plt.show()</code></pre>
    
    <h1>Support Vector Machine (SVM)</h1>
    <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.metrics import accuracy_score
import pandas as pd

# Read the data from the CSV file
data = pd.read_csv('Dataset.csv')

# Assuming the last column is the target variable and the rest are features
X = data.iloc[:, :-1]  # Features (all columns except the last)
y = data.iloc[:, -1]

# Generate synthetic data
# X, y = datasets.make_classification(n_samples=10, n_features=2, n_informative=2, n_redundant=0, random_state=42)
print(X)

print(f"y:{y}")
# Create an SVM classifier
clf = svm.SVC(kernel='linear')
clf.fit(X, y)

# Calculate and print accuracy
y_pred = clf.predict(X)
accuracy = accuracy_score(y, y_pred)
print("Accuracy:", accuracy)

# Print the number of support vectors
print(clf.support_vectors_)
num_support_vectors = len(clf.support_vectors_)
print("Number of support vectors:", num_support_vectors)

# Create a meshgrid to visualize the decision boundary
x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1
y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

# Plot the data points
plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm, s=30)

# Highlight support vectors
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')

# Set axis labels and title
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('SVM Decision Boundary')

# Show the plot
plt.show()</code></pre>
    
    <h1>Decision Tree</h1>
    <pre><code>import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

# Load data
data = pd.read_csv('Dataset.csv')
X = data.iloc[:, :-1]  # Features
y = data.iloc[:, -1]   # Target variable
print(X)
print("\n")
print(y)
print("\n")

# Decision Tree model
model = DecisionTreeClassifier()

# 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# Display accuracy for each fold
for fold, score in enumerate(scores, start=1):
    print(f"Accuracy for Fold {fold}: {score}")

# Mean accuracy across all folds
mean_accuracy = scores.mean()
print(f"\nMean Accuracy: {mean_accuracy}")</code></pre>
</body>
</html>
